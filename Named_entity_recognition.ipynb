{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    },
    "colab": {
      "name": "Named_entity_recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/humblethinker/ltsm-based-ner/blob/master/Named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5vQWc4tLUYU",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "\n",
        "The data will be mounted from drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIfceNJ4epQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9733d1c3-05b3-4bb3-bb40-2465d970c912"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8TqikliLUYg",
        "colab_type": "text"
      },
      "source": [
        "### Load the Twitter Named Entity Recognition corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwe1Zxq2LUYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "\n",
        "            if token.startswith('@'):\n",
        "                token = '<USR>'\n",
        "            elif token.lower().startswith('http://') or token.lower().startswith('https://'):\n",
        "                token = '<URL>'\n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-943jbBJLUYq",
        "colab_type": "text"
      },
      "source": [
        "And now we can load three separate parts of the dataset:\n",
        " - *train* data for training the model;\n",
        " - *validation* data for evaluation and hyperparameters tuning;\n",
        " - *test* data for final evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFRnIEHsLUYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens, train_tags = read_data('/content/drive/My Drive/data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('/content/drive/My Drive/data/train.txt')\n",
        "test_tokens, test_tags = read_data('/content/drive/My Drive/data/test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHyF2joALUY0",
        "colab_type": "text"
      },
      "source": [
        "Printing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_asMY2_LUY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2139f2f-6ec0-4fa6-e1f9-dc75542720e9"
      },
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT\tO\n",
            "<USR>\tO\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "<URL>\tO\n",
            "<URL>\tO\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "<USR>\tO\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Na1ynHWLUY-",
        "colab_type": "text"
      },
      "source": [
        "### Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings: \n",
        "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
        "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbAbU6NPLUY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw0mjAA7LUZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    vocab = set([l for ls in tokens_or_tags for l in ls])\n",
        "    vocab_size = len(vocab)+len(special_tokens)\n",
        "    idx2tok = ['']*vocab_size\n",
        "\n",
        "    for i,token in enumerate(special_tokens):\n",
        "        tok2idx[token] = i\n",
        "        idx2tok[i] = token\n",
        "    \n",
        "    for i, token in enumerate(vocab, len(special_tokens)):\n",
        "        tok2idx[token] = i\n",
        "        idx2tok[i] = token\n",
        "        \n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gROQifiILUZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFHaspUSLUZU",
        "colab_type": "text"
      },
      "source": [
        "The next additional functions will help us to create the mapping between tokens and ids for a sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjkOsPcLUZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fofvPbLKLUZY",
        "colab_type": "text"
      },
      "source": [
        "### Generate batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxDkeO3wLUZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOrU_2g_LUZd",
        "colab_type": "text"
      },
      "source": [
        "## Build a recurrent neural network\n",
        "\n",
        "This is the most important part of the project. Here we will specify the network architecture based on TensorFlow building blocks. We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K_W0K43LUZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ulJ2MCRLUZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMModel():\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "XNOHq-6RLUZn",
        "colab_type": "text"
      },
      "source": [
        "First, we need to create [placeholders](https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder) to specify what data we are going to feed into the network during the execution time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLfEpfyoLUZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    self.input_batch = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
        "  \n",
        "    self.lengths = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    self.dropout_ph = tf.compat.v1.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    self.learning_rate_ph = tf.compat.v1.placeholder(dtype=tf.float32, shape=[], name='learning_rate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyDioTnsLUZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "1CNUNtBmLUZv",
        "colab_type": "text"
      },
      "source": [
        "Now, let us specify the layers of the neural network. First, we need to perform some preparatory steps:\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qKlrS1_LUZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, name='embeddings_matrix', dtype=tf.float32)\n",
        "    \n",
        "    forward_cell =  tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units = n_hidden_rnn), input_keep_prob\t= self.dropout_ph, output_keep_prob\t= self.dropout_ph, state_keep_prob = self.dropout_ph)######### YOUR CODE HERE #############\n",
        "    backward_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units = n_hidden_rnn), input_keep_prob\t= self.dropout_ph, output_keep_prob\t= self.dropout_ph, state_keep_prob = self.dropout_ph) ######### YOUR CODE HERE #############\n",
        "\n",
        "    embeddings = tf.compat.v1.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
        "    \n",
        "    (rnn_output_fw, rnn_output_bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, self.lengths, dtype=tf.float32)\n",
        "    rnn_output = tf.compat.v1.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    self.logits = tf.compat.v1.layers.dense(rnn_output, n_tags, activation=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsV3dAeSLUZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__build_layers = classmethod(build_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vh6_oooLUZ5",
        "colab_type": "text"
      },
      "source": [
        "To compute the actual predictions of the neural network, we apply [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to the last layer and find the most probable tags with [argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy6HZXxZLUZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    softmax_output = tf.nn.softmax(self.logits)\n",
        "    \n",
        "    self.predictions = tf.math.argmax(softmax_output, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Jxuv3eLUaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OlPBdoCiLUaI",
        "colab_type": "text"
      },
      "source": [
        "We will use [cross-entropy loss](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), efficiently implemented in TF as \n",
        "[cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVSwXQaFLUaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    \n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
        "    \n",
        "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
        "  \n",
        "    self.loss = tf.reduce_mean(tf.multiply(loss_tensor, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV11A87BLUaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-wsG4U7LUaQ",
        "colab_type": "text"
      },
      "source": [
        "The last thing to specify is how we want to optimize the loss. \n",
        "We use [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) optimizer with a learning rate from the corresponding placeholder. \n",
        "We will also need to apply clipping to eliminate exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEfAbbp8LUaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    self.optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate_ph)\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "    \n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars =[ (tf.clip_by_norm(gv[0], clip_norm),gv[1]) for gv in self.grads_and_vars]\n",
        "    \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JrN8imhLUaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36W-1cDeLUaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VInUepFLUaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWomaRTVLUam",
        "colab_type": "text"
      },
      "source": [
        "## Train the network and predict tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23kfpe9_LUao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    session.run(self.train_op, feed_dict=feed_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvTdSjfALUat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPurDNhcLUa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_for_batch(self, session, x_batch, lengths):\n",
        "\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.dropout_ph: 1.0,\n",
        "                 self.lengths: lengths}\n",
        "\n",
        "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mhRcbBbLUa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgRuPgqfLUbF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1uC49gBLUbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluation import precision_recall_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEIPi5DOLUbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACu26IqYLUbS",
        "colab_type": "text"
      },
      "source": [
        "## Run your experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mzPMAtvLUbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])  ######### YOUR CODE HERE #############\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 7\n",
        "learning_rate = 0.005\n",
        "learning_rate_decay = np.sqrt(2)\n",
        "dropout_keep_probability = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OxtLJsLLUbd",
        "colab_type": "text"
      },
      "source": [
        "Finally, we are ready to run the training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDbk1yijLUbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a43a5e5e-7db7-406f-8779-76745d3185d5"
      },
      "source": [
        "sess = tf.compat.v1.Session()\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 71100 phrases; correct: 157.\n",
            "\n",
            "precision:  0.22%; recall:  3.50%; F1:  0.42\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 8589 phrases; correct: 25.\n",
            "\n",
            "precision:  0.29%; recall:  4.66%; F1:  0.55\n",
            "\n",
            "-------------------- Epoch 2 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 2488 phrases; correct: 470.\n",
            "\n",
            "precision:  18.89%; recall:  10.47%; F1:  13.47\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 221 phrases; correct: 40.\n",
            "\n",
            "precision:  18.10%; recall:  7.45%; F1:  10.55\n",
            "\n",
            "-------------------- Epoch 3 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4523 phrases; correct: 1698.\n",
            "\n",
            "precision:  37.54%; recall:  37.83%; F1:  37.68\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 315 phrases; correct: 122.\n",
            "\n",
            "precision:  38.73%; recall:  22.72%; F1:  28.64\n",
            "\n",
            "-------------------- Epoch 4 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4850 phrases; correct: 2592.\n",
            "\n",
            "precision:  53.44%; recall:  57.74%; F1:  55.51\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 415 phrases; correct: 155.\n",
            "\n",
            "precision:  37.35%; recall:  28.86%; F1:  32.56\n",
            "\n",
            "-------------------- Epoch 5 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 5054 phrases; correct: 3116.\n",
            "\n",
            "precision:  61.65%; recall:  69.41%; F1:  65.30\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 499 phrases; correct: 172.\n",
            "\n",
            "precision:  34.47%; recall:  32.03%; F1:  33.20\n",
            "\n",
            "-------------------- Epoch 6 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4840 phrases; correct: 3493.\n",
            "\n",
            "precision:  72.17%; recall:  77.81%; F1:  74.88\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 494 phrases; correct: 185.\n",
            "\n",
            "precision:  37.45%; recall:  34.45%; F1:  35.89\n",
            "\n",
            "-------------------- Epoch 7 of 7 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4717 phrases; correct: 3708.\n",
            "\n",
            "precision:  78.61%; recall:  82.60%; F1:  80.56\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 476 phrases; correct: 185.\n",
            "\n",
            "precision:  38.87%; recall:  34.45%; F1:  36.53\n",
            "\n",
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksimJx9cLUbg",
        "colab_type": "text"
      },
      "source": [
        "Now let us see full quality reports for the final model on train, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj-rQpZsLUbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96d9536a-4b27-4441-8cbd-bd6807483613"
      },
      "source": [
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False) ######### YOUR CODE HERE #############\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False) ######### YOUR CODE HERE #############"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4694 phrases; correct: 3780.\n",
            "\n",
            "precision:  80.53%; recall:  84.21%; F1:  82.33\n",
            "\n",
            "\t     company: precision:   90.50%; recall:   93.31%; F1:   91.88; predicted:   663\n",
            "\n",
            "\t    facility: precision:    7.32%; recall:    6.69%; F1:    6.99; predicted:   287\n",
            "\n",
            "\t     geo-loc: precision:   88.97%; recall:   96.39%; F1:   92.53; predicted:  1079\n",
            "\n",
            "\t       movie: precision:   40.68%; recall:   35.29%; F1:   37.80; predicted:    59\n",
            "\n",
            "\t musicartist: precision:   75.21%; recall:   78.45%; F1:   76.79; predicted:   242\n",
            "\n",
            "\t       other: precision:   81.77%; recall:   90.09%; F1:   85.73; predicted:   834\n",
            "\n",
            "\t      person: precision:   90.95%; recall:   96.39%; F1:   93.59; predicted:   939\n",
            "\n",
            "\t     product: precision:   84.40%; recall:   86.79%; F1:   85.58; predicted:   327\n",
            "\n",
            "\t  sportsteam: precision:   71.06%; recall:   76.96%; F1:   73.89; predicted:   235\n",
            "\n",
            "\t      tvshow: precision:   48.28%; recall:   24.14%; F1:   32.18; predicted:    29\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 465 phrases; correct: 186.\n",
            "\n",
            "precision:  40.00%; recall:  34.64%; F1:  37.13\n",
            "\n",
            "\t     company: precision:   62.22%; recall:   53.85%; F1:   57.73; predicted:    90\n",
            "\n",
            "\t    facility: precision:    7.41%; recall:    5.88%; F1:    6.56; predicted:    27\n",
            "\n",
            "\t     geo-loc: precision:   66.67%; recall:   51.33%; F1:   58.00; predicted:    87\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    11\n",
            "\n",
            "\t musicartist: precision:   14.71%; recall:   17.86%; F1:   16.13; predicted:    34\n",
            "\n",
            "\t       other: precision:   27.27%; recall:   33.33%; F1:   30.00; predicted:    99\n",
            "\n",
            "\t      person: precision:   43.06%; recall:   27.68%; F1:   33.70; predicted:    72\n",
            "\n",
            "\t     product: precision:   12.50%; recall:    8.82%; F1:   10.34; predicted:    24\n",
            "\n",
            "\t  sportsteam: precision:   20.00%; recall:   20.00%; F1:   20.00; predicted:    20\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 506 phrases; correct: 218.\n",
            "\n",
            "precision:  43.08%; recall:  36.09%; F1:  39.28\n",
            "\n",
            "\t     company: precision:   60.34%; recall:   41.67%; F1:   49.30; predicted:    58\n",
            "\n",
            "\t    facility: precision:    5.26%; recall:    4.26%; F1:    4.71; predicted:    38\n",
            "\n",
            "\t     geo-loc: precision:   72.50%; recall:   52.73%; F1:   61.05; predicted:   120\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     8\n",
            "\n",
            "\t musicartist: precision:   11.11%; recall:    7.41%; F1:    8.89; predicted:    18\n",
            "\n",
            "\t       other: precision:   27.41%; recall:   35.92%; F1:   31.09; predicted:   135\n",
            "\n",
            "\t      person: precision:   51.14%; recall:   43.27%; F1:   46.88; predicted:    88\n",
            "\n",
            "\t     product: precision:    9.09%; recall:    7.14%; F1:    8.00; predicted:    22\n",
            "\n",
            "\t  sportsteam: precision:   42.11%; recall:   25.81%; F1:   32.00; predicted:    19\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}